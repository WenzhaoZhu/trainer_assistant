Below is a step‐by‐step description of the (soft) EM algorithm for a document clustering model. In our example, we assume that each document is modeled as a bag of words and that there is a mixture of \(K\) clusters. For each cluster \(k\) we must infer:

1. **The Cluster Prior:**  
   \(\pi_k = P(z=k)\) (with \(\sum_{k=1}^K \pi_k = 1\)).

2. **The Cluster-Specific Word Distribution:**  
   \(\phi_{k} = \{\phi_{k,w}\}_{w=1}^{V}\), where \(V\) is the vocabulary size and \(\phi_{k,w} = P(\text{word } w \mid z=k)\) (with \(\sum_{w=1}^V \phi_{k,w} = 1\)).

Each document \(d_n\) (for \(n=1,\dots,N\)) is represented in a bag-of-words format. Let \(x_{n,w}\) denote the count of word \(w\) in document \(d_n\).

---

## **E-Step (Expectation Step)**

In the E-Step, we compute the posterior probability (or "responsibility") that document \(d_n\) was generated by cluster \(k\), given the current parameter estimates \(\theta^{(t)} = \{\pi_k^{(t)}, \phi_k^{(t)}\}\). Denote this probability by

\[
\gamma_{nk} \equiv P(z_n = k \mid d_n, \theta^{(t)}).
\]

Using Bayes’ rule, the formula is:

\[
\gamma_{nk} = \frac{\pi_k^{(t)}\,P(d_n \mid \phi_k^{(t)})}{\sum_{j=1}^K \pi_j^{(t)}\,P(d_n \mid \phi_j^{(t)})}.
\]

For a bag-of-words model (e.g., a multinomial model), the likelihood of document \(d_n\) under cluster \(k\) is given by

\[
P(d_n \mid \phi_k^{(t)}) = \prod_{w=1}^{V} \left(\phi_{k,w}^{(t)}\right)^{x_{n,w}}.
\]

Thus, the responsibilities become:

\[
\gamma_{nk} = \frac{\pi_k^{(t)} \prod_{w=1}^{V} \left(\phi_{k,w}^{(t)}\right)^{x_{n,w}}}{\sum_{j=1}^K \pi_j^{(t)} \prod_{w=1}^{V} \left(\phi_{j,w}^{(t)}\right)^{x_{n,w}}}.
\]

*Note:* In practice, it is common to work with logarithms to avoid numerical underflow when multiplying many probabilities.

---

## **M-Step (Maximization Step)**

In the M-Step, we update the parameters \(\pi_k\) and \(\phi_{k}\) to maximize the expected complete-data log-likelihood, where the expectation is taken with respect to the responsibilities \(\gamma_{nk}\) computed in the E-Step.

### **1. Updating the Cluster Priors \(\pi_k\)**

The updated cluster prior is computed as the normalized sum of responsibilities over all documents:

\[
\pi_k^{(t+1)} = \frac{1}{N} \sum_{n=1}^{N} \gamma_{nk}.
\]

This formula ensures that the priors remain a valid probability distribution (summing to 1).

### **2. Updating the Word Distributions \(\phi_{k}\)**

For each cluster \(k\) and each word \(w\) in the vocabulary, the updated parameter is the normalized expected count of word \(w\) in cluster \(k\):

\[
\phi_{k,w}^{(t+1)} = \frac{\sum_{n=1}^{N} \gamma_{nk}\, x_{n,w}}{\sum_{n=1}^{N} \gamma_{nk}\, \sum_{w'=1}^{V} x_{n,w'}}.
\]

Here:
- The numerator \(\sum_{n=1}^{N} \gamma_{nk}\, x_{n,w}\) is the expected count of word \(w\) in documents assigned (softly) to cluster \(k\).
- The denominator \(\sum_{n=1}^{N} \gamma_{nk}\, \left(\sum_{w'=1}^{V} x_{n,w'}\right)\) is the total expected number of words in cluster \(k\).

This update ensures that for each cluster \(k\), the word probabilities \(\phi_{k,w}\) sum to 1.

---

## **Document Assignment**

Even though the EM procedure produces soft assignments (\(\gamma_{nk}\) values between 0 and 1), you might sometimes wish to assign each document \(d_n\) to the cluster with the highest responsibility:

\[
\text{Cluster assignment for } d_n: \quad k^* = \arg\max_{k} \gamma_{nk}.
\]

This “hard” assignment is often used for final clustering results, even though the parameter estimation was performed using soft assignments.

---

## **Summary of the EM Steps**

1. **E-Step:**  
   For each document \(d_n\) and cluster \(k\):
   \[
   \gamma_{nk} = \frac{\pi_k^{(t)} \prod_{w=1}^{V} \left(\phi_{k,w}^{(t)}\right)^{x_{n,w}}}{\sum_{j=1}^K \pi_j^{(t)} \prod_{w=1}^{V} \left(\phi_{j,w}^{(t)}\right)^{x_{n,w}}}.
   \]

2. **M-Step:**  
   Update the parameters as follows:
   - **Cluster Priors:**
     \[
     \pi_k^{(t+1)} = \frac{1}{N} \sum_{n=1}^{N} \gamma_{nk}.
     \]
   - **Word Distributions:**
     \[
     \phi_{k,w}^{(t+1)} = \frac{\sum_{n=1}^{N} \gamma_{nk}\, x_{n,w}}{\sum_{n=1}^{N} \gamma_{nk}\, \sum_{w'=1}^{V} x_{n,w'}}.
     \]

3. **Optional Hard Assignments:**  
   For each document \(d_n\), assign:
   \[
   k^* = \arg\max_{k} \gamma_{nk}.
   \]

These steps are iterated until convergence (i.e., until the parameters change very little between iterations or the log-likelihood plateaus).

This is the complete EM procedure for document clustering in a (soft)-EM context.